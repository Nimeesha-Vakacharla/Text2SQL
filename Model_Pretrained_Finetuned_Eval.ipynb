{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "101f6677-af47-4765-b8d0-2a72d484fbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.19.11)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (6.30.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.11.4)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.27.0)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /opt/conda/lib/python3.10/site-packages (from wandb) (4.13.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d6afdfa-5586-441f-a004-61782933ba45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft==0.15.0 accelerate>=0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba525c74-f58d-4fa7-a157-8c824a79152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d20867-26e0-4406-8b26-8a720be604d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.2)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.12.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.31.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (20.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3f6407-83c4-4352-9621-14a3ac18e622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlparse in /opt/conda/lib/python3.10/site-packages (0.5.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sqlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f88adfd-b277-4a09-83d5-831a277b0b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in /opt/conda/lib/python3.10/site-packages (1.4.54)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy) (3.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "594107cf-101f-47b8-bf2f-798b3ce95ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import sqlparse\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For database execution\n",
    "from sqlalchemy import create_engine, text\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f4c9769-9aca-484d-a338-205d6cb3736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Configuration and constants\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "DATASET_NAME = \"xlangai/spider\"\n",
    "OUTPUT_DIR = \"./llama3-text2sql\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    report_to=\"wandb\"  # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39f3ae58-931d-4ab2-95da-f04ad92c01d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dataset in /opt/conda/lib/python3.10/site-packages (1.6.2)\n",
      "Requirement already satisfied: sqlalchemy<2.0.0,>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from dataset) (1.4.54)\n",
      "Requirement already satisfied: alembic>=0.6.2 in /opt/conda/lib/python3.10/site-packages (from dataset) (1.15.2)\n",
      "Requirement already satisfied: banal>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from dataset) (1.0.6)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=0.6.2->dataset) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /opt/conda/lib/python3.10/site-packages (from alembic>=0.6.2->dataset) (4.13.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<2.0.0,>=1.3.2->dataset) (3.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=0.6.2->dataset) (2.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0188a5-2148-4634-a649-eef711e489aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4a: Load schemas separately\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"spider\")\n",
    "from sqlalchemy import create_engine\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "def get_schema(db_id):\n",
    "    # You'll need to download the SQLite database files from Spider\n",
    "    # and place them in a 'database' folder\n",
    "    try:\n",
    "        conn = sqlite3.connect(f'database/{db_id}/{db_id}.sqlite')\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get tables and columns\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = cursor.fetchall()\n",
    "        \n",
    "        schema_info = []\n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "            cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "            columns = [col[1] for col in cursor.fetchall()]\n",
    "            schema_info.append(f\"Table {table_name}: {', '.join(columns)}\")\n",
    "        \n",
    "        conn.close()\n",
    "        return \"\\n\".join(schema_info)\n",
    "    except:\n",
    "        return \"Schema not available\"\n",
    "\n",
    "# Cell 5a: Modified with schema\n",
    "def preprocess_with_schema(example):\n",
    "    schema = get_schema(example['db_id'])\n",
    "    \n",
    "    prompt = f\"\"\"Translate this question to SQL using the database schema.\n",
    "    \n",
    "Database Schema:\n",
    "{schema}\n",
    "\n",
    "Question: {example['question']}\n",
    "SQL Query:\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"completion\": example['query'],\n",
    "        \"db_id\": example['db_id']\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "train_dataset = dataset['train'].map(preprocess_with_schema)\n",
    "eval_dataset = dataset['validation'].map(preprocess_with_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7f04094-418d-46f1-a29f-ac8919d5230e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.45.5)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.0->bitsandbytes) (12.9.41)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dbb5377-df19-4825-99b3-2eac662ea0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9580e6d6654398924ec15add112d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 54,525,952 || all params: 8,084,787,200 || trainable%: 0.6744\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load tokenizer and model with authentication\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Authenticate with your Hugging Face token\n",
    "login(token=\"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\")  # Replace with your actual token\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=\"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\"  # Add token here as well for redundancy\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # for batch inference\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=\"your_hf_token_here\"  # Add token here\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "054681d2-a9ea-4528-a43e-7efaf856db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Data collator and tokenization\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize prompts and completions\n",
    "    tokenized_prompts = tokenizer(examples[\"prompt\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    tokenized_completions = tokenizer(examples[\"completion\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    \n",
    "    # Combine and create labels (ignore prompt tokens in loss calculation)\n",
    "    input_ids = [p + c for p, c in zip(tokenized_prompts[\"input_ids\"], tokenized_completions[\"input_ids\"])]\n",
    "    attention_mask = [p + c for p, c in zip(tokenized_prompts[\"attention_mask\"], tokenized_completions[\"attention_mask\"])]\n",
    "    labels = [[-100] * len(p) + c for p, c in zip(tokenized_prompts[\"input_ids\"], tokenized_completions[\"input_ids\"])]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "321dc3b2-efc0-4a21-935b-9a3ac24c22af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpsindhu1905\u001b[0m (\u001b[33mpsindhu1905-san-jose-state-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/app/wandb/run-20250510_132036-bciqkh3z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psindhu1905-san-jose-state-university/huggingface/runs/bciqkh3z' target=\"_blank\">./llama3-text2sql</a></strong> to <a href='https://wandb.ai/psindhu1905-san-jose-state-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psindhu1905-san-jose-state-university/huggingface' target=\"_blank\">https://wandb.ai/psindhu1905-san-jose-state-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psindhu1905-san-jose-state-university/huggingface/runs/bciqkh3z' target=\"_blank\">https://wandb.ai/psindhu1905-san-jose-state-university/huggingface/runs/bciqkh3z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1311' max='1311' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1311/1311 2:48:05, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.085089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.106944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./llama3-text2sql\n",
      "Final model saved as model.pt in ./llama3-text2sql\n",
      "***** train metrics *****\n",
      "  epoch                    =      2.9943\n",
      "  total_flos               = 679976828GF\n",
      "  train_loss               =      0.1222\n",
      "  train_runtime            =  2:48:14.76\n",
      "  train_samples_per_second =        2.08\n",
      "  train_steps_per_second   =        0.13\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Optimized Fine-tuning\n",
    "from transformers import Trainer\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Enable faster training optimizations\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "os.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n",
    "\n",
    "# Create optimized trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training with progress monitoring\n",
    "try:\n",
    "    print(\"Starting training...\")\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Save only the adapter weights to save time/space\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    print(f\"Model saved to {OUTPUT_DIR}\")\n",
    "    \n",
    "    # Save the final fine-tuned model in .pt file\n",
    "    torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"model.pt\"))\n",
    "    print(f\"Final model saved as model.pt in {OUTPUT_DIR}\")\n",
    "    \n",
    "    # Save training metrics\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted. Saving current progress...\")\n",
    "    trainer.save_model(OUTPUT_DIR + \"_interrupted\")\n",
    "    print(\"Partial model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "632351d1-cde1-4f50-8664-7cf2f34e6f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Evaluation metrics setup\n",
    "# Load metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    bleu_score = bleu.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=[[label] for label in decoded_labels]\n",
    "    )[\"bleu\"]\n",
    "    \n",
    "    # Compute exact match\n",
    "    em_score = exact_match.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels\n",
    "    )[\"exact_match\"]\n",
    "    \n",
    "    # Compute SQL similarity (simplified)\n",
    "    sql_similarity = []\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        try:\n",
    "            # Parse and format SQL\n",
    "            pred_sql = sqlparse.format(pred, reindent=True, keyword_case='upper')\n",
    "            label_sql = sqlparse.format(label, reindent=True, keyword_case='upper')\n",
    "            sql_similarity.append(pred_sql == label_sql)\n",
    "        except:\n",
    "            sql_similarity.append(False)\n",
    "    \n",
    "    sql_accuracy = np.mean(sql_similarity)\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"exact_match\": em_score,\n",
    "        \"sql_accuracy\": sql_accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c3ecebe-d22c-4631-9301-670d597e8fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating pretrained model: 100%|██████████| 100/100 [02:22<00:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "BLEU: 0.3925\n",
      "Exact Match: 0.0700\n",
      "SQL Accuracy: 0.0700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 (Final Corrected Version): Evaluation with fixed syntax\n",
    "def evaluate_pretrained_model(sample_size=100):\n",
    "    # Select evaluation samples\n",
    "    eval_samples = eval_dataset.select(range(sample_size))\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    for example in tqdm(eval_samples, desc=\"Evaluating pretrained model\"):\n",
    "        try:\n",
    "            prompt = example[\"prompt\"]\n",
    "            \n",
    "            # Tokenize with proper formatting\n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(model.device)\n",
    "            \n",
    "            # Generate output\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_new_tokens=256,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode only the generated part\n",
    "            generated_sql = tokenizer.decode(\n",
    "                outputs[0][inputs[\"input_ids\"].shape[1]:], \n",
    "                skip_special_tokens=True\n",
    "            ).strip()\n",
    "            \n",
    "            predictions.append(generated_sql)\n",
    "            references.append(example[\"completion\"])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example: {str(e)}\")\n",
    "            predictions.append(\"\")\n",
    "            references.append(example[\"completion\"])\n",
    "    \n",
    "    # Compute metrics with proper syntax\n",
    "    def safe_compute_metrics(preds, refs):\n",
    "        # Convert all inputs to strings\n",
    "        preds = [str(p) for p in preds]\n",
    "        refs = [[str(r)] for r in refs]\n",
    "        \n",
    "        # Calculate BLEU\n",
    "        try:\n",
    "            bleu_score = bleu.compute(predictions=preds, references=refs)[\"bleu\"]\n",
    "        except:\n",
    "            bleu_score = 0.0\n",
    "            \n",
    "        # Calculate Exact Match\n",
    "        try:\n",
    "            em_score = sum(1 for p, r in zip(preds, refs) if p == r[0]) / len(preds)\n",
    "        except:\n",
    "            em_score = 0.0\n",
    "            \n",
    "        # Calculate SQL Accuracy (fixed syntax)\n",
    "        try:\n",
    "            sql_acc = sum(1 for p, r in zip(preds, refs) \n",
    "                      if sqlparse.format(p) == sqlparse.format(r[0])) / len(preds)\n",
    "        except:\n",
    "            sql_acc = 0.0\n",
    "            \n",
    "        return {\n",
    "            \"bleu\": bleu_score,\n",
    "            \"exact_match\": em_score,\n",
    "            \"sql_accuracy\": sql_acc\n",
    "        }\n",
    "    \n",
    "    metrics = safe_compute_metrics(predictions, references)\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(f\"BLEU: {metrics['bleu']:.4f}\")\n",
    "    print(f\"Exact Match: {metrics['exact_match']:.4f}\")\n",
    "    print(f\"SQL Accuracy: {metrics['sql_accuracy']:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Run evaluation\n",
    "pretrained_metrics = evaluate_pretrained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1d99f99-c6d4-4819-94ee-d70fb58a0078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fine-tuned model: 100%|██████████| 50/50 [03:36<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuned Model Evaluation Metrics:\n",
      "BLEU: 0.5158\n",
      "Exact Match: 0.2400\n",
      "SQL Accuracy: 0.2400\n",
      "\n",
      "Example 1:\n",
      "Predicted: SELECT count(*) FROM singer\n",
      "Reference: SELECT count(*) FROM singer\n",
      "\n",
      "Example 2:\n",
      "Predicted: SELECT count(*) FROM singer\n",
      "Reference: SELECT count(*) FROM singer\n",
      "\n",
      "Example 3:\n",
      "Predicted: SELECT name, country, age FROM singer ORDER BY age DESC\n",
      "Reference: SELECT name, country, age FROM singer ORDER BY age DESC\n",
      "***** eval metrics *****\n",
      "  bleu         = 0.5158\n",
      "  exact_match  =   0.24\n",
      "  sql_accuracy =   0.24\n"
     ]
    }
   ],
   "source": [
    "import sqlparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from evaluate import load\n",
    "\n",
    "def evaluate_finetuned_model(trainer, eval_dataset, tokenizer, sample_size=50):\n",
    "    eval_samples = eval_dataset.select(range(min(sample_size, len(eval_dataset))))\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    trainer.model.eval()\n",
    "    \n",
    "    for example in tqdm(eval_samples, desc=\"Evaluating fine-tuned model\"):\n",
    "        try:\n",
    "            prompt = example[\"prompt\"]\n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(trainer.model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = trainer.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_new_tokens=512,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    num_beams=5,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "            \n",
    "            generated_sql = tokenizer.decode(\n",
    "                outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip()\n",
    "            \n",
    "            predictions.append(generated_sql)\n",
    "            references.append(example[\"completion\"])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {prompt}: {str(e)}\")\n",
    "            predictions.append(\"SELECT * FROM table\")  # Fallback query\n",
    "            references.append(example[\"completion\"])\n",
    "    \n",
    "    # Normalize SQL\n",
    "    def normalize_sql(sql):\n",
    "        try:\n",
    "            sql = sqlparse.format(sql, reindent=True, keyword_case='upper', strip_comments=True)\n",
    "            sql = ' '.join(sql.split())\n",
    "            return sql.rstrip(';')\n",
    "        except:\n",
    "            return sql\n",
    "    \n",
    "    preds = [normalize_sql(p) for p in predictions]\n",
    "    refs = [[normalize_sql(r)] for r in references]\n",
    "    \n",
    "    # Compute metrics\n",
    "    exact_match = load(\"exact_match\")\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    # Compute BLEU score using NLTK\n",
    "    bleu_scores = []\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        pred_tokens = pred.split()\n",
    "        ref_tokens = [r.split() for r in ref]  # refs is a list of lists\n",
    "        bleu_scores.append(sentence_bleu(ref_tokens, pred_tokens, smoothing_function=smoothing))\n",
    "    bleu_score = np.mean(bleu_scores)\n",
    "    \n",
    "    em_score = exact_match.compute(predictions=preds, references=[r[0] for r in refs])[\"exact_match\"]\n",
    "    \n",
    "    sql_similarity = []\n",
    "    for pred, ref in zip(preds, [r[0] for r in refs]):\n",
    "        sql_similarity.append(pred == ref)\n",
    "    sql_accuracy = np.mean(sql_similarity)\n",
    "    \n",
    "    metrics = {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"exact_match\": em_score,\n",
    "        \"sql_accuracy\": sql_accuracy\n",
    "    }\n",
    "    \n",
    "    print(\"\\nFine-tuned Model Evaluation Metrics:\")\n",
    "    print(f\"BLEU: {metrics['bleu']:.4f}\")\n",
    "    print(f\"Exact Match: {metrics['exact_match']:.4f}\")\n",
    "    print(f\"SQL Accuracy: {metrics['sql_accuracy']:.4f}\")\n",
    "    \n",
    "    # Print sample predictions\n",
    "    for i, (pred, ref) in enumerate(zip(preds[:3], [r[0] for r in refs][:3])):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Predicted: {pred}\")\n",
    "        print(f\"Reference: {ref}\")\n",
    "    \n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Run evaluation\n",
    "finetuned_metrics = evaluate_finetuned_model(trainer, tokenized_eval, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45055a45-3f66-445c-8d34-5215291e1d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Text-to-SQL Query Interface\n",
      "==================================================\n",
      "You can enter up to 3 natural language questions about the database.\n",
      "Type 'exit' to quit early.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter question 1 of 3:  Which statuses correspond to both cities that have a population over 1500 and cities that have a population lower than 500?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Question: Which statuses correspond to both cities that have a population over 1500 and cities that have a population lower than 500?\n",
      "Generated SQL: SELECT Status FROM city WHERE Population  >  1500 INTERSECT SELECT Status FROM city WHERE Population  <  500;\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter question 2 of 3:  What details do we have on the students who registered for courses most recently?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Question: What details do we have on the students who registered for courses most recently?\n",
      "Generated SQL: SELECT T1.student_details FROM Students AS T1 JOIN Student_Course_Registration AS T2 ON T1.student_id  =  T2.student_id ORDER BY T2.registration_date DESC LIMIT 1;\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter question 3 of 3:  What are the ids of the candidates that have an outcome code of Pass?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Question: What are the ids of the candidates that have an outcome code of Pass?\n",
      "Generated SQL: SELECT candidate_id FROM candidate_outcomes WHERE outcome_code  =  'Pass';\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Thank you for using the Text-to-SQL interface!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 (Interactive Inference Function with 3-question limit)\n",
    "import sqlite3\n",
    "import sqlparse\n",
    "\n",
    "def interactive_text_to_sql(db_path=None):\n",
    "    \"\"\"\n",
    "    Interactive interface for text-to-SQL conversion (3 questions max)\n",
    "    Args:\n",
    "        db_path: Optional path to SQLite database for execution\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Text-to-SQL Query Interface\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"You can enter up to 3 natural language questions about the database.\")\n",
    "    print(\"Type 'exit' to quit early.\\n\")\n",
    "    \n",
    "    question_count = 0\n",
    "    \n",
    "    while question_count < 3:\n",
    "        # Get user input\n",
    "        question = input(f\"\\nEnter question {question_count + 1} of 3: \").strip()\n",
    "        \n",
    "        if question.lower() in ['exit', 'quit']:\n",
    "            print(\"\\nExiting text-to-SQL interface early...\")\n",
    "            break\n",
    "            \n",
    "        if not question:\n",
    "            print(\"Please enter a valid question.\")\n",
    "            continue\n",
    "            \n",
    "        # Generate SQL\n",
    "        prompt = f\"\"\"Translate the following natural language question into SQL query.\n",
    "        \n",
    "Question: {question}\n",
    "SQL Query:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Generate SQL\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Extract and clean SQL\n",
    "            generated_sql = tokenizer.decode(\n",
    "                outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip()\n",
    "            generated_sql = sqlparse.format(generated_sql.split(\";\")[0] + \";\")\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*50)\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Generated SQL: {generated_sql}\")\n",
    "            \n",
    "            # Execute if database path provided\n",
    "            if db_path:\n",
    "                try:\n",
    "                    conn = sqlite3.connect(db_path)\n",
    "                    cursor = conn.cursor()\n",
    "                    cursor.execute(generated_sql)\n",
    "                    results = cursor.fetchall()\n",
    "                    columns = [desc[0] for desc in cursor.description] if cursor.description else []\n",
    "                    conn.close()\n",
    "                    \n",
    "                    if results:\n",
    "                        print(\"\\nQuery Results:\")\n",
    "                        # Print column headers\n",
    "                        print(\" | \".join(columns))\n",
    "                        print(\"-\" * (sum(len(col) for col in columns) + 3*len(columns)))\n",
    "                        # Print rows\n",
    "                        for row in results:\n",
    "                            print(\" | \".join(str(x) for x in row))\n",
    "                    else:\n",
    "                        print(\"\\nQuery executed successfully but returned no results.\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError executing query: {str(e)}\")\n",
    "            \n",
    "            print(\"-\"*50 + \"\\n\")\n",
    "            \n",
    "            question_count += 1  # Only increment on successful question processing\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError generating SQL: {str(e)}\")\n",
    "    \n",
    "    print(\"\\nThank you for using the Text-to-SQL interface!\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment and provide your database path if available\n",
    "    # interactive_text_to_sql(db_path=\"your_database.db\")\n",
    "    interactive_text_to_sql()  # Without database execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673464c4-c12e-4ae6-b2a8-e12cc4d3c0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
